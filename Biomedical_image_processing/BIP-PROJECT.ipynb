{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df74c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from keras import Sequential\n",
    "from skimage import exposure\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import filters\n",
    "from scipy.fft import fftn, fftshift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_image(pdfile, column):\n",
    "    classes = sorted(list(pdfile[column].unique()))\n",
    "    groups = pdfile.groupby(column)\n",
    "    print('{0:^12s} {1:^12s}'.format('CLASS', 'IMAGE COUNT'))\n",
    "    count_list = []\n",
    "    label_list = []\n",
    "    for i in classes:\n",
    "        group = groups.get_group(i)\n",
    "        count_list.append(len(group))\n",
    "        label_list.append(i)\n",
    "        print('{0:^12s} {1:^12s}'.format(i, str(len(group))))\n",
    "    max_value = np.max(count_list)\n",
    "    max_index = count_list.index(max_value)\n",
    "    max_class = label_list[max_index]\n",
    "    min_value = np.min(count_list)\n",
    "    min_index = count_list.index(min_value)\n",
    "    min_class = label_list[min_index]\n",
    "    print(max_class, ' has the most images= ', max_value, ' ', min_class, ' has the least images= ', min_value)\n",
    "\n",
    "def trim(df, max_samples, min_samples, column):\n",
    "    df_copy = df.copy()\n",
    "    groups = df_copy.groupby(column)\n",
    "    trimmed_df = pd.DataFrame(columns= df_copy.columns)\n",
    "    for label in df_copy[column].unique():\n",
    "        group = groups.get_group(label)\n",
    "        count = len(group)\n",
    "        if count > max_samples:\n",
    "            sampled_group = group.sample(n = max_samples, random_state = 123, axis = 0)\n",
    "            trimmed_df = pd.concat([trimmed_df, sampled_group], axis= 0)\n",
    "        elif count >= min_samples:\n",
    "            sampled_group = group\n",
    "            trimmed_df = pd.concat([trimmed_df, sampled_group], axis = 0)\n",
    "    print(f\"The number of max sample in any class is now: {max_samples}, \\n \"\n",
    "          f\"The number of min sample in any class is now: {min_samples}\")\n",
    "    return trimmed_df\n",
    "\n",
    "def Data_augmentation(df, n, working_directory, img_size, column):\n",
    " \n",
    "    df = df.copy()\n",
    "    print(f\"Initial length of the data: {len(df)}\")\n",
    "    aug_dir = os.path.join(working_directory, 'aug')\n",
    "    if os.path.isdir(aug_dir):\n",
    "        shutil.rmtree(aug_dir) # Delete the existing directory\n",
    "    os.mkdir(aug_dir) # Create the new directory\n",
    "    for label in df[column].unique():\n",
    "        dir_path = os.path.join(aug_dir, label) # Create a new subdirectory for each label\n",
    "        os.mkdir(dir_path)\n",
    "    total = 0\n",
    "    gen = ImageDataGenerator(horizontal_flip= True, rotation_range = 30,\n",
    "                             brightness_range= [0.3, 0.8],\n",
    "                             width_shift_range=0.2, height_shift_range= 0.2,\n",
    "                             validation_split= 0.2, zoom_range= 0.3)\n",
    "    groups = df.groupby(column)\n",
    "    for label in df[column].unique():\n",
    "        group = groups.get_group(label)\n",
    "        count = len(group)\n",
    "        if count < n:\n",
    "            aug_img_count = 0\n",
    "            Needed_to_be_augmented = n - count\n",
    "            target_dir = os.path.join(aug_dir, label)\n",
    "            aug_gen = gen.flow_from_dataframe(group, x_col= 'Data_file', y_col= None,\n",
    "                                              target_size= img_size, class_mode= None, batch_size= 1,\n",
    "                                              shuffle= False, save_to_dir= target_dir, save_prefix= 'aug',\n",
    "                                              color_mode= 'rgb',save_format= 'jpg')\n",
    "            while aug_img_count < Needed_to_be_augmented:\n",
    "                images = next(aug_gen)\n",
    "                aug_img_count += len(images)\n",
    "            total += aug_img_count\n",
    "    print(f\"Total amount of augmentation: {total} images\")\n",
    "    \n",
    "    aug_fpaths = []\n",
    "    aug_labels = []\n",
    "    classlist = os.listdir(aug_dir)\n",
    "    for kclass in classlist:\n",
    "        classpath = os.path.join(aug_dir, kclass)\n",
    "        flist = os.listdir(classpath)\n",
    "        for f in flist:\n",
    "            fpath = os.path.join(classpath, f)\n",
    "            aug_fpaths.append(fpath)\n",
    "            aug_labels.append(kclass)\n",
    "    Image_data = pd.Series(aug_fpaths, name= 'Data_file')\n",
    "    Labels = pd.Series(aug_labels, name= 'Classification')\n",
    "    aug_df  = pd.concat([Image_data, Labels], axis= 1)\n",
    "    df = pd.concat([df, aug_df], axis= 0).reset_index(drop= True)\n",
    "    return df\n",
    "\n",
    "def Translate_data(df):\n",
    "    Data = []\n",
    "    Classes_list = [\"Healthy\", \"Doubtful\", \"Minimal\", \"Moderate\", \"Severe\"]\n",
    "    Label = df.loc[:, 'Classification'].map({\"Healthy\": 0, \"Doubtful\": 1,\n",
    "                                             \"Minimal\": 2, \"Moderate\": 3,\n",
    "                                             \"Severe\": 4})\n",
    "    for i in df['Data_file']:\n",
    "        img = cv2.imread(i)\n",
    "        Data.append(img)\n",
    "        Data_1 = np.stack(Data, axis= 0)\n",
    "    return Data_1,  Label\n",
    "\n",
    "def Image_Preprocessing_1(data):\n",
    "    # each filter we work with each line of filter in this code\n",
    "    Filtered_data = []\n",
    "    low = 0.03\n",
    "    high = 0.3\n",
    "    for i in range(data.shape[0]):\n",
    "        img = data[i, :, :, :]\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #DoG\n",
    "        #img = cv2.equalizeHist(gray_img)\n",
    "        #arr = np.zeros_like(img)\n",
    "        #arr = difference_of_gaussians(img, low_sigma= 1.5, high_sigma= 15)\n",
    "        #arr_combined_1 = exposure.rescale_intensity(arr, in_range=(0,1), out_range=(0, 1))\n",
    "        #sobel\n",
    "        #edges = filters.sobel(gray_img)\n",
    "        \n",
    "        #Hysteresis thresholding\n",
    "        #edges = filters.sobel(gray_img)\n",
    "        #hyst = filters.apply_hysteresis_threshold(edges, low, high)\n",
    "        \n",
    "        Filtered_data.append(gray_img)\n",
    "    Filtered_data = np.stack(Filtered_data, axis= 0)\n",
    "    return Filtered_data\n",
    "\n",
    "def model4():\n",
    "    model4 = Sequential()\n",
    "    model4.add(layers.Conv2D(190, 5, activation= 'relu', input_shape= (224,224, 1)))\n",
    "    model4.add(layers.Conv2D(30, 5, activation = 'relu'))\n",
    "    model4.add(layers.BatchNormalization())\n",
    "    model4.add(layers.MaxPool2D(2))\n",
    "    model4.add(layers.Conv2D(130, 1, activation='relu'))\n",
    "    model4.add(layers.BatchNormalization())\n",
    "    model4.add(layers.MaxPool2D(2))\n",
    "    model4.add(layers.Dropout(0.5))\n",
    "    model4.add(layers.Flatten())\n",
    "    model4.add(layers.Dense(5, activation='softmax'))\n",
    "    model4.compile(optimizer=keras.optimizers.Adam(learning_rate= 1e-05),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics='accuracy')\n",
    "    return model4\n",
    "\n",
    "def show_image(data):\n",
    "    t_dict = data.class_indices\n",
    "    classes = list(t_dict.keys())\n",
    "    images, labels = next(data)\n",
    "    plt.figure(figsize= (20, 20))\n",
    "    length = len(labels)\n",
    "    if length < 10:\n",
    "        r = length\n",
    "    else:\n",
    "        r = 10\n",
    "    for i in range(r):\n",
    "        plt.subplots(5, 5, i+1)\n",
    "        image = images[i] /255\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])\n",
    "        class_name = classes[index]\n",
    "        plt.title(class_name, color = 'blue', fontsize = 12, loc= 'center')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"D:/STUDY/IMG_PROCESS/Histogram\"\n",
    "train_path = os.path.join(base_dir,'train')\n",
    "valid_path = os.path.join(base_dir,'val')\n",
    "test_path = os.path.join(base_dir, 'test')\n",
    "list_of_classes = [\"Healthy\", \"Doubtful\", \"Minimal\", \"Moderate\", \"Severe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05291382",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train_path,test_path,valid_path]:\n",
    "    file_path = []\n",
    "    labels = []\n",
    "    classlist = os.listdir(d)\n",
    "    for klass in classlist:\n",
    "        intklass = int(klass)\n",
    "        label = list_of_classes[intklass]\n",
    "        classpath = os.path.join(d, klass)\n",
    "        flist = os.listdir(classpath)\n",
    "        for f in flist:\n",
    "            fpath = os.path.join(classpath, f)\n",
    "            file_path.append(fpath)\n",
    "            labels.append(label)\n",
    "    Data = pd.Series(file_path, name='Data_file')\n",
    "    Classes = pd.Series(labels, name= 'Classification')\n",
    "    pdf = pd.concat([Data, Classes], axis= 1)\n",
    "    if d == test_path:\n",
    "        test_pdf = pdf\n",
    "    elif d == valid_path:\n",
    "        val_pdf = pdf\n",
    "    else:\n",
    "        train_pdf = pdf\n",
    "print(f\"the length of train data {len(train_pdf)} \\n \"f\"The length of test data {len(test_pdf)} \\n \"\n",
    "      f\"The length of val data {len(val_pdf)}\")\n",
    "print(train_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Classification'\n",
    "count_image(train_pdf, column)\n",
    "count_image(test_pdf, column)\n",
    "count_image(val_pdf, column)\n",
    "train_pdf_1 = trim(train_pdf, max_samples= 700, min_samples= 173, column= column)\n",
    "count_image(train_pdf, column)\n",
    "train_pdf_2 = Data_augmentation(path = r\"D:/STUDY/IMG_PROCESS/Histogram\")\n",
    "print(train_pdf_2)\n",
    "Data_train, label_data_train = Translate_data(train_pdf_2)\n",
    "Data_test, label_data_test = Translate_data(test_pdf)\n",
    "Data_val, label_data_val = Translate_data(val_pdf)\n",
    "print(Data_train.shape)\n",
    "Filtered_data = Image_Preprocessing_1(Data_train)\n",
    "Filtered_data_val = Image_Preprocessing_1(Data_val)\n",
    "Filtered_data_test = Image_Preprocessing_1(Data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data_train = np.array(label_data_train)\n",
    "label_data_test = np.array(label_data_test)\n",
    "label_data_val = np.array(label_data_val)\n",
    "unique_train, count_train = np.unique(label_data_train, return_counts= True)\n",
    "print(np.asarray((unique_train, count_train)).T)\n",
    "unique_test, count_test = np.unique(label_data_test, return_counts= True)\n",
    "print(np.asarray((unique_test, count_test)).T)\n",
    "label_data_train_new = tf.one_hot(label_data_train, 5)\n",
    "label_data_test_new = tf.one_hot(label_data_test, 5)\n",
    "label_data_val_new = tf.one_hot(label_data_val, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model4()\n",
    "history_1= model_1.fit(Filtered_data, label_data_train_new, epochs = 20,verbose= 0, batch_size= 70,\n",
    "                    validation_data=(Filtered_data_val, label_data_val_new)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54152409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model('Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model1.evaluate(Filtered_data_test, label_data_test_new, verbose = 1)\n",
    "print(f\"The accuracy of evaluation: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model1.predict(Filtered_data_test, verbose= 0, batch_size= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.argmax(y_predict, axis = -1)\n",
    "#Visualize the accuracy of training and validation\n",
    "plt.plot(history_1.history['accuracy'])\n",
    "plt.plot(history_1.history['val_accuracy'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the loss of training and validation\n",
    "plt.plot(history_1.history['loss'])\n",
    "plt.plot(history_1.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c84434",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = accuracy_score(y_true=label_data_test,y_pred= y_predict)\n",
    "print(f\"The accuracy: {Accuracy}\")\n",
    "print(f\"The classification report of the model: \\n {classification_report(y_pred= y_predict, y_true= label_data_test)}\")\n",
    "y_predict = np.argmax(y_predict, axis= -1)\n",
    "cm = confusion_matrix(label_data_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcba06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = y_predict.reshape(-1,1)\n",
    "from sklearn.preprocessing import LabelEncoder as le\n",
    "## Get Class Labels\n",
    "le = le()\n",
    "le.fit_transform(y_predict)\n",
    "labels = le.classes_\n",
    "class_names = labels\n",
    "print(class_names)\n",
    "# Plot confusion matrix in a beautiful manner\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g') #annot=True to annotate cells\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "ax.xaxis.set_label_position('bottom')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.xaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "ax.xaxis.tick_bottom()\n",
    "\n",
    "ax.set_ylabel('True', fontsize=20)\n",
    "ax.yaxis.set_ticklabels(class_names, fontsize = 10)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Refined Confusion Matrix', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import save_model\n",
    "#Save the model as h5 file\n",
    "file_name = \"Model_hysteresis.h5\"\n",
    "file_path = os.path.join(os.getcwd(), file_name)\n",
    "save_model(model_1, file_path)\n",
    "print(\"Saved the Model to the disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
